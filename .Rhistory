total_holding = sum((1 + r) * holdings)
wealth_tracker[i] = total_holding
current_holding = total_holding
}
all_simulation = rbind(all_simulation, wealth_tracker)
}
wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 1000, xlim = c(-10000, 10000),
main = "Gain for Portofolio 3")
abline(v = quantile(wealth_gain, prob=0.05), col = 'red')
text(quantile(wealth_gain, prob=0.05) - 1500, 275, "VaR (5%)", col = 'red')
hist(wealth_gain, breaks = 1000, xlim = c(-10000, 10000),
main = "Gain for Portofolio 3")
abline(v = quantile(wealth_gain, prob=0.05), col = 'red')
text(quantile(wealth_gain, prob=0.05) + 1500, 600, "VaR (5%)", col = 'red')
quantile(wealth_gain, prob=0.05)
quantile(wealth_gain, prob=0.05)
quantile(wealth_gain, prob=0.05)
rm(list = ls())
library(LICORS)
df = read.csv('social_marketing.csv', header = T)
df = df[, -1]
clusters = c(2: 15)
within_SSE = rep(0, length(clusters))
between_SSE = rep(0, length(clusters))
for (i in c(1: length(clusters)))
{
clust1 = kmeanspp(df, clusters[i])
within_SSE[i] = clust1$tot.withinss
between_SSE[i] = clust1$betweenss
}
plot(clusters, within_SSE)
setwd("C:/Studies/Intro to ML/group assignment 2")
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(LICORS)
df = read.csv('social_marketing.csv', header = T)
df = df[, -1]
clusters = c(2: 15)
within_SSE = rep(0, length(clusters))
between_SSE = rep(0, length(clusters))
for (i in c(1: length(clusters)))
{
clust1 = kmeanspp(df, clusters[i])
within_SSE[i] = clust1$tot.withinss
between_SSE[i] = clust1$betweenss
}
plot(clusters, within_SSE)
plot(clusters, between_SSE)
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
final_cluster = clust1
cluster_mean = t(data.frame(final_cluster$centers))
cluster_mean
# 20% cutoff  is taken
cluster1 = cluster_mean[abs(cluster_mean[,1]) >=  0.2*max(abs(cluster_mean[,1])), 1]
barplot(cluster1)
cluster2 = cluster_mean[abs(cluster_mean[,2]) >=  0.2*max(abs(cluster_mean[,2])), 2]
barplot(cluster2)
cluster3 = cluster_mean[abs(cluster_mean[,3]) >=  0.2*max(abs(cluster_mean[,3])), 3]
barplot(cluster3)
cluster4 = cluster_mean[abs(cluster_mean[,4]) >=  0.2*max(abs(cluster_mean[,4])), 4]
barplot(cluster4)
barplot(cluster1, horiz = T)
barplot(cluster1, horiz = T, las = 2)
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
final_cluster = clust1
cluster_mean = t(data.frame(final_cluster$centers))
cluster_mean
# 20% cutoff  is taken
cluster1 = cluster_mean[abs(cluster_mean[,1]) >=  0.2*max(abs(cluster_mean[,1])), 1]
barplot(cluster1, horiz = T, las = 2)
cluster2 = cluster_mean[abs(cluster_mean[,2]) >=  0.2*max(abs(cluster_mean[,2])), 2]
barplot(cluster2)
cluster3 = cluster_mean[abs(cluster_mean[,3]) >=  0.2*max(abs(cluster_mean[,3])), 3]
barplot(cluster3)
cluster4 = cluster_mean[abs(cluster_mean[,4]) >=  0.2*max(abs(cluster_mean[,4])), 4]
barplot(cluster4)
barplot(cluster1, las = 2)
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
final_cluster = clust1
cluster_mean = t(data.frame(final_cluster$centers))
cluster_mean
par(mfrow=c(2,2), mar=c(1,1,1,1))
# 20% cutoff  is taken
cluster1 = cluster_mean[abs(cluster_mean[,1]) >=  0.2*max(abs(cluster_mean[,1])), 1]
barplot(cluster1, las = 2)
cluster2 = cluster_mean[abs(cluster_mean[,2]) >=  0.2*max(abs(cluster_mean[,2])), 2]
barplot(cluster2)
cluster3 = cluster_mean[abs(cluster_mean[,3]) >=  0.2*max(abs(cluster_mean[,3])), 3]
barplot(cluster3)
cluster4 = cluster_mean[abs(cluster_mean[,4]) >=  0.2*max(abs(cluster_mean[,4])), 4]
barplot(cluster4)
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
final_cluster = clust1
cluster_mean = t(data.frame(final_cluster$centers))
cluster_mean
par(mfrow=c(2,2), mar=c(2,2,2,2))
# 20% cutoff  is taken
cluster1 = cluster_mean[abs(cluster_mean[,1]) >=  0.2*max(abs(cluster_mean[,1])), 1]
barplot(cluster1, las = 2)
cluster2 = cluster_mean[abs(cluster_mean[,2]) >=  0.2*max(abs(cluster_mean[,2])), 2]
barplot(cluster2, las = 2)
cluster3 = cluster_mean[abs(cluster_mean[,3]) >=  0.2*max(abs(cluster_mean[,3])), 3]
barplot(cluster3, las = 2)
cluster4 = cluster_mean[abs(cluster_mean[,4]) >=  0.2*max(abs(cluster_mean[,4])), 4]
barplot(cluster4, las = 2)
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
final_cluster = clust1
cluster_mean = t(data.frame(final_cluster$centers))
cluster_mean
par(mfrow=c(2,2), mar=c(2,2,2,2))
# 20% cutoff  is taken
cluster1 = cluster_mean[abs(cluster_mean[,1]) >=  0.2*max(abs(cluster_mean[,1])), 1]
barplot(cluster1, las = 2)
cluster2 = cluster_mean[abs(cluster_mean[,2]) >=  0.2*max(abs(cluster_mean[,2])), 2]
barplot(cluster2, las = 2)
cluster3 = cluster_mean[abs(cluster_mean[,3]) >=  0.2*max(abs(cluster_mean[,3])), 3]
barplot(cluster3, las = 2)
cluster4 = cluster_mean[abs(cluster_mean[,4]) >=  0.2*max(abs(cluster_mean[,4])), 4]
barplot(cluster4, las = 2)
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
final_cluster = clust1
cluster_mean = t(data.frame(final_cluster$centers))
cluster_mean
cluster1 = cluster_mean[abs(cluster_mean[,1]) >=  0.2*max(abs(cluster_mean[,1])), 1]
cluster2 = cluster_mean[abs(cluster_mean[,2]) >=  0.2*max(abs(cluster_mean[,2])), 2]
cluster3 = cluster_mean[abs(cluster_mean[,3]) >=  0.2*max(abs(cluster_mean[,3])), 3]
cluster4 = cluster_mean[abs(cluster_mean[,4]) >=  0.2*max(abs(cluster_mean[,4])), 4]
par(mfrow=c(2,2), mar=c(2,2,2,2))
# 20% cutoff  is taken
barplot(cluster1, las = 2)
barplot(cluster2, las = 2)
barplot(cluster3, las = 2)
barplot(cluster4, las = 2)
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
final_cluster = clust1
cluster_mean = t(data.frame(final_cluster$centers))
cluster_mean
cluster1 = cluster_mean[abs(cluster_mean[,1]) >=  0.2*max(abs(cluster_mean[,1])), 1]
cluster2 = cluster_mean[abs(cluster_mean[,2]) >=  0.2*max(abs(cluster_mean[,2])), 2]
cluster3 = cluster_mean[abs(cluster_mean[,3]) >=  0.2*max(abs(cluster_mean[,3])), 3]
cluster4 = cluster_mean[abs(cluster_mean[,4]) >=  0.2*max(abs(cluster_mean[,4])), 4]
par(mfrow=c(2,2), mar=c(2,2,2,2))
# 20% cutoff  is taken
barplot(cluster1, las = 2)
barplot(cluster2, las = 2)
barplot(cluster3, las = 2)
barplot(cluster4, las = 2)
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
final_cluster = clust1
cluster_mean = t(data.frame(final_cluster$centers))
cluster_mean
cluster1 = cluster_mean[abs(cluster_mean[,1]) >=  0.2*max(abs(cluster_mean[,1])), 1]
cluster2 = cluster_mean[abs(cluster_mean[,2]) >=  0.2*max(abs(cluster_mean[,2])), 2]
cluster3 = cluster_mean[abs(cluster_mean[,3]) >=  0.2*max(abs(cluster_mean[,3])), 3]
cluster4 = cluster_mean[abs(cluster_mean[,4]) >=  0.2*max(abs(cluster_mean[,4])), 4]
par(mfrow=c(4,1), mar=c(2,2,2,2))
# 20% cutoff  is taken
barplot(cluster1, las = 2, horiz = T)
barplot(cluster2, las = 2, horiz = T)
barplot(cluster3, las = 2, horiz = T)
barplot(cluster4, las = 2, horiz = T)
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
final_cluster = clust1
cluster_mean = t(data.frame(final_cluster$centers))
cluster_mean
cluster1 = cluster_mean[abs(cluster_mean[,1]) >=  0.2*max(abs(cluster_mean[,1])), 1]
cluster2 = cluster_mean[abs(cluster_mean[,2]) >=  0.2*max(abs(cluster_mean[,2])), 2]
cluster3 = cluster_mean[abs(cluster_mean[,3]) >=  0.2*max(abs(cluster_mean[,3])), 3]
cluster4 = cluster_mean[abs(cluster_mean[,4]) >=  0.2*max(abs(cluster_mean[,4])), 4]
par(mfrow=c(4,1), mar=c(2,2,2,2))
# 20% cutoff  is taken
barplot(cluster1, las = 2, horiz = T)
barplot(cluster2, las = 2, horiz = T)
barplot(cluster3, las = 2, horiz = T)
barplot(cluster4, las = 2, horiz = T)
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
final_cluster = clust1
cluster_mean = t(data.frame(final_cluster$centers))
cluster_mean
cluster1 = cluster_mean[abs(cluster_mean[,1]) >=  0.2*max(abs(cluster_mean[,1])), 1]
cluster2 = cluster_mean[abs(cluster_mean[,2]) >=  0.2*max(abs(cluster_mean[,2])), 2]
cluster3 = cluster_mean[abs(cluster_mean[,3]) >=  0.2*max(abs(cluster_mean[,3])), 3]
cluster4 = cluster_mean[abs(cluster_mean[,4]) >=  0.2*max(abs(cluster_mean[,4])), 4]
par(mfrow=c(4,1), mar=c(2,2,2,2))
# 20% cutoff  is taken
barplot(cluster1, las = 2, horiz = T)
barplot(cluster2, las = 2, horiz = T)
barplot(cluster3, las = 2, horiz = T)
barplot(cluster4, las = 2, horiz = T)
length(which(clust1$cluster == 1))
length(which(clust1$cluster == 2))
length(which(clust1$cluster == 3))
length(which(clust1$cluster == 4))
hier_people = hclust(dist(df, method='euclidean'), method='complete')
plot(hier_people, cex=0.8)
cluster2 = cutree(hier_people, k = 4)
summary(factor(cluster2))
rm(list = ls())
library(tm)
library(tidyverse)
library(slam)
library(proxy)
library(plyr)
library(caret)
library(randomForest)
# function for reading text as plain data
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
# fetching all directory
file_list = Sys.glob('./ReutersC50/C50train/*/*.txt')
# read text
text = lapply(file_list, readerPlain)
# Create a corpus from the text above
documents_raw = Corpus(VectorSource(text))
# modifying corpus
my_documents = documents_raw %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
# removing stop words
my_documents = tm_map(my_documents,
content_transformer(removeWords),
stopwords("en"))
# Create Term frequency matrix. Each row is term frequncy vector of single  document
DTM = DocumentTermMatrix(my_documents)
# Current Sparisty is 99%. Let's remove long tail of infrequently used words
DTM = removeSparseTerms(DTM, 0.95)
train_X = as.matrix(DTM)
# Let's create target variable
train_Y = rep(1, 50)
for (i in 2:50)
{
train_Y = c(train_Y, rep(i, 50))
}
# fetching all directory
file_list = Sys.glob('./ReutersC50/C50test/*/*.txt')
# read text
text = lapply(file_list, readerPlain)
# Create a corpus from the text above
documents_raw = Corpus(VectorSource(text))
# modifying corpus
my_documents = documents_raw %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
# removing stop words
my_documents = tm_map(my_documents,
content_transformer(removeWords),
stopwords("en"))
# Create Term frequency matrix. Each row is term frequncy vector of single  document
DTM = DocumentTermMatrix(my_documents)
# Current Sparisty is 99%. Let's remove long tail of infrequently used words
DTM = removeSparseTerms(DTM, 0.975)
test_X = as.matrix(DTM)
# Let's create target variable
test_Y = rep(1, 50)
for (i in 2:50)
{
test_Y = c(test_Y, rep(i, 50))
}
scrub_cols = which(colSums(train_X) == 0)
scrub_cols = which(sapply(as.data.frame(train_X), function(x) var(x, na.rm = T)) == 0)
train_X = train_X[,-scrub_cols]
pr.out=prcomp(train_X , scale=TRUE, rank = 200)
summary(pr.out)
rf = randomForest(x = pr.out$x, y = as.factor(train_Y),
type = 'classification', maxnodes = 300)
table = confusionMatrix(rf$predicted, as.factor(train_Y),
positive = NULL,
dnn = c("Prediction", "Reference"))
View(train_X)
View(train_X)
View(table)
table[["byClass"]]
hm <- table$table %>% gather(x, value, 1:50)
hm <- as.data.frame(table$table) %>% gather(x, value, 1:50)
as.data.frame(table$table)
as.data.frame(table$table) %>% ggplot(aes(x=prediction, y=reference, fill=freq)) + geom_tile()
as.data.frame(table$table)
as.data.frame(table$table) %>% ggplot(aes(x=Prediction, y=Reference, fill=Freq)) + geom_tile()
cols = c()
for (i in c(1: length(test_Y)))
{
if(colnames(test_X)[i] %in% colnames(train_X))
cols = c(cols, i)
}
test_scrubbed = scale(test_X)[,cols]
yy <- read.table(textConnection(""), col.names = colnames(train_X),
colClasses = "integer")
test_data = rbind.fill(yy, as.data.frame(test_scrubbed))
test_data[is.na(test_data)] = 0
test_pca = predict(pr.out, newdata = test_data)
test_result = predict(rf, newdata = test_pca, type = 'class')
table = confusionMatrix(test_result, as.factor(train_Y),
positive = NULL,
dnn = c("Prediction", "Reference"))
as.data.frame(table$table) %>% ggplot(aes(x=Prediction, y=Reference, fill=Freq)) + geom_tile()
table$overall
rf = randomForest(x = pr.out$x, y = as.factor(train_Y),
type = 'classification', maxnodes = 300)
table = confusionMatrix(rf$predicted, as.factor(train_Y),
positive = NULL,
dnn = c("Prediction", "Reference"))
print(table$overall)
as.data.frame(table$table) %>% ggplot(aes(x=Prediction, y=Reference, fill=Freq)) + geom_tile()
cols = c()
for (i in c(1: length(test_Y)))
{
if(colnames(test_X)[i] %in% colnames(train_X))
cols = c(cols, i)
}
test_scrubbed = scale(test_X)[,cols]
yy <- read.table(textConnection(""), col.names = colnames(train_X),
colClasses = "integer")
test_data = rbind.fill(yy, as.data.frame(test_scrubbed))
test_data[is.na(test_data)] = 0
test_pca = predict(pr.out, newdata = test_data)
test_result = predict(rf, newdata = test_pca, type = 'class')
table = confusionMatrix(test_result, as.factor(train_Y),
positive = NULL,
dnn = c("Prediction", "Reference"))
print(table$overall)
as.data.frame(table$table) %>% ggplot(aes(x=Prediction, y=Reference, fill=Freq)) + geom_tile()
rm(list = ls())
library(arules)
library(arulesViz)
groceries = read.transactions('groceries.txt', sep = ',',
rm.duplicates = T, header = F,
format = 'basket')
summary(groceries)
View(groceries)
x = arules::inspect(subset(grocery_rules, subset=lift > 3.2 & support < 0.1))
grocery_rules = apriori(groceries,
parameter=list(support=.005, confidence=.1, maxlen=5))
x = arules::inspect(subset(grocery_rules, subset=lift > 3.2 & support < 0.1))
plot(grocery_rules, measure = c("support", "confidence"), shading = 'lift')
x = arules::inspect(subset(grocery_rules, subset=lift > 2 & support < 0.1))
plot(x, measure = c("support", "confidence"), shading = 'lift')
x = subset(grocery_rules, subset=lift > 2 & support < 0.1)
plot(x, measure = c("support", "confidence"), shading = 'lift')
plot(x, measure = c("support", "confidence"), shading = 'lift',
xlim = c(0, 0.3))
x = subset(grocery_rules, subset=lift > 2 & support < 0.1)
inspect(x)
subset_rules =  inspect(x)
x = subset(grocery_rules, subset=lift > 2 & support < 0.1)
inspect(x)
head(inspect(x))
x = subset(grocery_rules, subset=lift > 2 & support < 0.1)
head(inspect(x))
saveAsGraph(head(subset(grocery_rules, subset=lift > 2 & support < 0.1), n = 1000, by = "lift"), file = "grocery.graphml")
knitr::include_graphics("gephi.png")
plot(x, method = 'graph')
plot(head(x, 100, by='lift'), method='graph')
plot(head(x, 50, by='lift'), method='graph')
plot(head(x, 40, by='lift'), method='graph')
plot(head(x, 40, by='lift'), method='graph', main = "title")
rm(list = ls())
library(DescTools)
library(tidyverse)
df = read.csv('greenbuildings.csv', header = T)
df$green_rating = as.factor(df$green_rating)
df$scaled_rent = df$Rent / df$cluster_rent
#scaled
difference_in_rents =
median(df[df$green_rating  == 1,]$scaled_rent) -
median(df[df$green_rating  == 0,]$scaled_rent)
difference_in_rents
additional_pay_from_green
additional_rent_pre_sqft = 25 * (difference_in_rents + 1) #assuming $25 to be base
area = 250000 #sqft
additional_pay_from_green = additional_rent_pre_sqft * area
additional_pay_from_green
additional_pay_from_green
#df %>% ggplot(aes(x = stories, y = Rent)) +
#  geom_point() +
#  geom_vline(xintercept = 15) +
#  stat_smooth(method = "lm", col = "red", formula = 'y ~ x') +
#  facet_wrap(vars(green_rating)) + xlim(0, 60)
#
#df %>% ggplot(aes(x = size, y = Rent, color = class_a)) +
#  geom_point() +
#  geom_vline(xintercept = 250000) +
#  stat_smooth(method = "lm", col = "red", formula = 'y ~ x') +
#  facet_wrap(vars(green_rating)) + scale_x_log10() + ylim(0, 100)
df$class_a = as.factor(df$class_a)
df %>% ggplot(aes(x = Rent, color = green_rating)) +
geom_histogram() +  facet_wrap(vars(class_a), nrow = 2) +
xlim(0, 100) +
labs(title = "Facet wrap - Class A")
df %>% ggplot(aes(x = Rent, color = green_rating)) +
geom_histogram() +
facet_wrap(vars(amenities), nrow = 2) +
xlim(0, 100) +
labs(title = "Facet wrap - Amenties")
df1 = df[df$class_a == 1,]
additional_rent_pre_sqft = median(df1[df1$green_rating  == 1,]$Rent) - median(df1[df1$green_rating  == 0,]$Rent)
area = 250000 #sqft
additional_pay_from_green = additional_rent_pre_sqft * area
additional_pay_from_green
additional_pay_from_green
df2 = df[df$amenities == 1,]
additional_rent_pre_sqft = median(df2[df2$green_rating  == 1,]$Rent) - median(df2[df2$green_rating  == 0,]$Rent)
area = 250000 #sqft
additional_pay_from_green = additional_rent_pre_sqft * area
additional_pay_from_green
additional_pay_from_green
df3 = df[df$amenities == 1 & df$class_a == 1,]
additional_rent_pre_sqft = median(df3[df3$green_rating  == 1,]$Rent) - median(df3[df3$green_rating  == 0,]$Rent)
area = 250000 #sqft
additional_pay_from_green = additional_rent_pre_sqft * area
additional_pay_from_green
rm(list = ls())
library(DescTools)
library(tidyverse)
green.buildings = read.csv('greenbuildings.csv')
summary(green.buildings)
dim(green.buildings)
# removing occupancy less than 10% to be inline with the analysis done
green.buildings = subset(green.buildings, leasing_rate >= 10)
dim(green.buildings)
rent_agg = green.buildings %>% group_by(green_rating) %>% summarise(m = median(Rent))
colnames(rent_agg) = c('green_rating', 'median_rent')
# removing occupancy less than 10% to be inline with the analysis done
green.buildings = subset(green.buildings, leasing_rate >= 10)
dim(green.buildings)
rent_agg = green.buildings %>% group_by(green_rating) %>% summarise(m = median(Rent))
colnames(rent_agg) = c('green_rating', 'median_rent')
View(rent_agg)
green.buildings$green_rating = as.factor(green.buildings$green_rating)
rent_agg = green.buildings %>% group_by(green_rating) %>% summarise(m = median(Rent))
# removing occupancy less than 10% to be inline with the analysis done
green.buildings = subset(green.buildings, leasing_rate >= 10)
dim(green.buildings)
green.buildings$green_rating = as.factor(green.buildings$green_rating)
rm(list = ls())
library(DescTools)
library(tidyverse)
green.buildings = read.csv('greenbuildings.csv')
summary(green.buildings)
dim(green.buildings)
# removing occupancy less than 10% to be inline with the analysis done
green.buildings = subset(green.buildings, leasing_rate >= 10)
dim(green.buildings)
green.buildings$green_rating = as.factor(green.buildings$green_rating)
rent_agg = green.buildings %>% group_by(green_rating) %>% summarise(m = median(Rent))
View(green.buildings)
rm(list = ls())
library(DescTools)
library(tidyverse)
green.buildings = read.csv('greenbuildings.csv')
summary(green.buildings)
dim(green.buildings)
# removing occupancy less than 10% to be inline with the analysis done
green.buildings = subset(green.buildings, leasing_rate >= 10)
dim(green.buildings)
green.buildings$green_rating = as.factor(green.buildings$green_rating)
rent_agg = green.buildings %>% group_by(green_rating) %>% summarise(m = median(Rent))
colnames(rent_agg) = c('green_rating', 'median_rent')
green.buildings %>% group_by(green_rating) %>% summarise(m = median(Rent))
green.buildings %>% group_by(green_rating) %>% summarise(m = median(Rent))
green.buildings %>% group_by(green_rating)
rent_agg = green.buildings %>% group_by(green_rating) %>% summarise(m = median(Rent))
dim(green.buildings)
# removing occupancy less than 10% to be inline with the analysis done
green.buildings = subset(green.buildings, leasing_rate >= 10)
# removing occupancy less than 10% to be inline with the analysis done
green.buildings = subset(green.buildings, leasing_rate >= 10)
dim(green.buildings)
green.buildings$green_rating = as.factor(green.buildings$green_rating)
rent_agg = green.buildings %>% group_by(green_rating) %>% summarise(m = median(Rent))
colnames(rent_agg) = c('green_rating', 'median_rent')
# removing occupancy less than 10% to be inline with the analysis done
green.buildings = subset(green.buildings, leasing_rate >= 10)
green.buildings = read.csv('greenbuildings.csv')
summary(green.buildings)
dim(green.buildings)
# removing occupancy less than 10% to be inline with the analysis done
green.buildings = subset(green.buildings, leasing_rate >= 10)
dim(green.buildings)
View(green.buildings)
green.buildings$green_rating = as.factor(green.buildings$green_rating)
rent_agg = green.buildings %>%
group_by(green_rating) %>%
summarise(m = median(Rent))
green.buildings %>%
group_by(green_rating) %>%
summarise(m = median(Rent))
