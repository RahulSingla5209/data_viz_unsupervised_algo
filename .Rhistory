test$Purchase1 = rep(0,length(test$Purchase))
test$Purchase1[test$Purchase=='Yes'] = 1
test$AVRAAUT  = NULL
test$PVRAAUT  = NULL
test$Purchase = NULL
yhat=predict(logistic ,newdata =test,type='response')
predicted = rep(0,length(test$Purchase1))
predicted[yhat>0.2] = 1
table(test$Purchase1,predicted)
set.seed(45)
train = Caravan[1:1000,]
test = Caravan[1001:5822,]
train$Purchase1 = rep(0,length(train$Purchase))
train$Purchase1[train$Purchase=='Yes'] = 1
train$AVRAAUT  = NULL
train$PVRAAUT  = NULL
train$Purchase = NULL
logistic = glm(Purchase1~., data=train,family='binomial')
test$Purchase1 = rep(0,length(test$Purchase))
test$Purchase1[test$Purchase=='Yes'] = 1
test$AVRAAUT  = NULL
test$PVRAAUT  = NULL
test$Purchase = NULL
yhat=predict(logistic ,newdata =test,type='response')
predicted = rep(0,length(test$Purchase1))
predicted[yhat>0.2] = 1
table(test$Purchase1,predicted)
cat("\n")
cat("Fraction of people predicted to make a purchase and actually make one:", 58/(58+350))
cat("\n")
cat("Accuracy:", (4183+58)/(58+231+4183+350))
midcity = read.csv('MidCity.csv')
midcity$Nbhd = as.factor(midcity$Nbhd)
midcity$Brick_num = ifelse(midcity$Brick=='Yes',1,0)
set.seed(45)
linear_regression_model = lm(Price~SqFt+Bedrooms+Offers+Bathrooms+Nbhd+Brick_num,data=midcity)
summary(linear_regression_model)
summary(linear_regression_model)
set.seed(45)
nonlinear_regression_model = lm(Price~SqFt+Bedrooms+Offers+Bathrooms+Nbhd+Brick_num+Brick_num*Nbhd,data=midcity)
summary(nonlinear_regression_model)
set.seed(45)
midcity$Nbhd = as.numeric(midcity$Nbhd)
midcity$Nbhd_3 = ifelse(midcity$Nbhd==3,1,0)
linear_regression_model = lm(Price~SqFt+Bedrooms+Offers+Bathrooms+Nbhd_1+Nbhd_2+Brick_num,data=midcity)
set.seed(45)
midcity$Nbhd = as.numeric(midcity$Nbhd)
midcity$Nbhd_3 = ifelse(midcity$Nbhd==3,1,0)
linear_regression_model = lm(Price~SqFt+Bedrooms+Offers+Bathrooms+Nbhd_3+Brick_num,data=midcity)
summary(linear_regression_model)
###standardize the train x's
library(nnet)
set.seed(45)
minv = rep(0,13)
maxv = rep(0,13)
bostonsc = Boston
for(i in 1:13) {
minv[i] = min(Boston[[i+1]])
maxv[i] = max(Boston[[i+1]])
bostonsc[[i+1]] = (Boston[[i+1]]-minv[i])/(maxv[i]-minv[i])
}
final_size = NULL
final_decay = NULL
size_list = c(3,5,7,10)
decay_list = c(.1,.001,.0001)
k_folds = 5
n_train = dim(Boston)[1]
n0 = round(n_train/k_folds, 0)
out_MSE = matrix(0, k_folds)
train_cv_df = data.frame(matrix(nrow=0, ncol=3))
for(size in size_list)
for (decay in decay_list){
{
used = NULL
set = 1:n_train
for(j in 1:k_folds){
if(n0<length(set)){val = sample(set,n0)}
if(n0>=length(set)){val=set}
model = nnet(crim~.,bostonsc[-val,],size=size,decay=decay,trace=F)
bostonsc_test = bostonsc[,-c(1)]
prediction = predict(model, bostonsc[val,])
out_MSE[j] = mean((prediction - (bostonsc[val,]$crim))^2)
used = union(used,val)
set = (1:n_train)[-used]
}
avg_rmse_os = mean(out_MSE)
final_size = size
final_decay = decay
train_cv_df = rbind(train_cv_df,cbind(avg_rmse_os,final_size,final_decay))
}
}
cat("\n","\n")
cat('Min MSE:',min(train_cv_df$avg_rmse_os),' and Size:',
train_cv_df[which(train_cv_df$avg_rmse_os==min(train_cv_df$avg_rmse_os)),]$final_size
,' and Decay:',
train_cv_df[which(train_cv_df$avg_rmse_os==min(train_cv_df$avg_rmse_os)),]$final_decay)
train_cv
train_cv_df
###standardize the train x's
library(nnet)
set.seed(45)
minv = rep(0,13)
maxv = rep(0,13)
bostonsc = Boston
for(i in 1:13) {
minv[i] = min(Boston[[i+1]])
maxv[i] = max(Boston[[i+1]])
bostonsc[[i+1]] = (Boston[[i+1]]-minv[i])/(maxv[i]-minv[i])
}
final_size = NULL
final_decay = NULL
size_list = c(3,5,7,10)
decay_list = c(.1,.001,.0001)
k_folds = 5
n_train = dim(Boston)[1]
n0 = round(n_train/k_folds, 0)
out_MSE = matrix(0, k_folds)
train_cv_df = data.frame(matrix(nrow=0, ncol=3))
for(size in size_list)
for (decay in decay_list){
{
used = NULL
set = 1:n_train
for(j in 1:k_folds){
if(n0<length(set)){val = sample(set,n0)}
if(n0>=length(set)){val=set}
model = nnet(crim~.,bostonsc[-val,],size=size,decay=decay,trace=F)
bostonsc_test = bostonsc[,-c(1)]
prediction = predict(model, bostonsc[val,])
out_MSE[j] = mean((prediction - (bostonsc[val,]$crim))^2)
used = union(used,val)
set = (1:n_train)[-used]
}
avg_rmse_os = mean(out_MSE)
final_size = size
final_decay = decay
train_cv_df = rbind(train_cv_df,cbind(avg_rmse_os,final_size,final_decay))
}
}
cat("\n","\n")
cat('Min MSE:',min(train_cv_df$avg_rmse_os),' and Size:',
train_cv_df[which(train_cv_df$avg_rmse_os==min(train_cv_df$avg_rmse_os)),]$final_size
,' and Decay:',
train_cv_df[which(train_cv_df$avg_rmse_os==min(train_cv_df$avg_rmse_os)),]$final_decay)
train_cv_df['hyparameter'] = paste(train_cv_df$final_size, train_cv_df$final_decay, sep="_")
plot(train_cv_df$avg_rmse_os,train_cv_df$avg_rmse_os$hyperparameter,xlab='hyperparameters',ylab='avg_rmse_oos',main='Comparison of Avg RMSE vs Hyperparameter',col='red')
train_cv_df
plot(train_cv_df$avg_rmse_os,train_cv_df$hyperparameter,xlab='hyperparameters',ylab='avg_rmse_oos',main='Comparison of Avg RMSE vs Hyperparameter',col='red')
View(cv_tree_model)
load("~/.RData")
load("~/.RData")
setwd("C:/Users/prakh/Desktop/GitHub/STA380/data/ReutersC50/C50train")
library(tm)
library(tidyverse)
library(slam)
library(proxy)
library(glmnet)
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
file_list = Sys.glob('*/*.txt')
authors = lapply(file_list, readerPlain)
author_names = Sys.glob('*')
# Clean up the file names
# no doubt the stringr library would be nicer here.
# this is just what I hacked together
mynames = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
# Rename the articles
names(authors) = mynames
## once you have documents in a vector, you
## create a text mining 'corpus' with:
documents_raw = Corpus(VectorSource(authors))
## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus
my_documents = documents_raw %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
## Remove stopwords.  Always be careful with this: one person's trash is another one's treasure.
# 2 example built-in sets of stop words
stopwords("en")
stopwords("SMART")
?stopwords
# let's just use the "basic English" stop words
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
## create a doc-term-matrix from the corpus
DTM_authors = DocumentTermMatrix(my_documents)
DTM_authors # some basic summary statistics
## You can inspect its entries...
inspect(DTM_authors[1:10,1:20])
## ...find words with greater than a min count...
findFreqTerms(DTM_authors, 50)
## Finally, let's drop those terms that only occur in one or two documents
## This is a common step: the noise of the "long tail" (rare terms)
## can be huge, and there is nothing to learn if a term occured once.
## Below removes those terms that have count 0 in >95% of docs.
## Probably a bit stringent here... but only 50 docs!
DTM_authors = removeSparseTerms(DTM_authors, 0.95)
DTM_authors # now ~ 1000 terms (versus ~3000 before)
# construct TF IDF weights -- might be useful if we wanted to use these
# as features in a predictive model
tfidf_authors = weightTfIdf(DTM_authors)
X = as.matrix(tfidf_authors)
summary(colSums(X))
scrub_cols = which(colSums(X) == 0)
X = X[,-scrub_cols]
pca_authors = prcomp(X, scale=TRUE)
pve = summary(pca_authors)$importance[3,]
plot(pve)  # not much of an elbow
## 75% variance is explained at PC 331
X_pca = pca_authors$x[,1:331]
y = rep(author_names,each=50)
## Random Forest Classifier
library(randomForest)
randomForest(y~X_pca,X_pca)
train = cbind(X_pca,y)
randomForest(y~.,train)
train = as.data.frame(cbind(X_pca,y))
View(train)
train$y = as.factor(train$y)
rf = randomForest(y~.,train)
rf = randomForest(y~.,train,ntree=100,mtry=2,importance=TRUE)
rf = randomForest(y~.,data=train,ntree=100,mtry=2,importance=TRUE)
rf = randomForest(y~.,data=train,ntree=100,mtry=2,importance=TRUE)
rf = randomForest(y ~ .,data=train,ntree=100,mtry=2,importance=TRUE)
rf_classifier = randomForest(y ~ .,data=train,ntree=100,mtry=2,importance=TRUE)
rf_classifier <- randomForest(y ~ .,data=train,ntree=100,mtry=2,importance=TRUE)
randomForest(y~.,train)
rf_classifier <- randomForest(y ~ .,data=train,ntree=100,mtry=2,importance=TRUE)
predict(rf_classifier)
rf_classifier <- randomForest(y ~ .,data=train,ntree=100,mtry=2,importance=TRUE)
rf_classifier <- randomForest(y ~ .,data=train,ntree=100,mtry=2,importance=TRUE)
train = as.data.frame(cbind(X_pca,y))
setwd("C:/Users/prakh/Desktop/GitHub/STA380/data/ReutersC50/C50train")
library(tm)
library(tidyverse)
library(slam)
library(proxy)
library(glmnet)
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
file_list = Sys.glob('*/*.txt')
authors = lapply(file_list, readerPlain)
author_names = Sys.glob('*')
# Clean up the file names
# no doubt the stringr library would be nicer here.
# this is just what I hacked together
mynames = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
# Rename the articles
names(authors) = mynames
## once you have documents in a vector, you
## create a text mining 'corpus' with:
documents_raw = Corpus(VectorSource(authors))
## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus
my_documents = documents_raw %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
## Remove stopwords.  Always be careful with this: one person's trash is another one's treasure.
# 2 example built-in sets of stop words
stopwords("en")
stopwords("SMART")
?stopwords
# let's just use the "basic English" stop words
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
## create a doc-term-matrix from the corpus
DTM_authors = DocumentTermMatrix(my_documents)
DTM_authors # some basic summary statistics
## You can inspect its entries...
inspect(DTM_authors[1:10,1:20])
## ...find words with greater than a min count...
findFreqTerms(DTM_authors, 50)
## Finally, let's drop those terms that only occur in one or two documents
## This is a common step: the noise of the "long tail" (rare terms)
## can be huge, and there is nothing to learn if a term occured once.
## Below removes those terms that have count 0 in >95% of docs.
## Probably a bit stringent here... but only 50 docs!
DTM_authors = removeSparseTerms(DTM_authors, 0.95)
DTM_authors # now ~ 1000 terms (versus ~3000 before)
# construct TF IDF weights -- might be useful if we wanted to use these
# as features in a predictive model
tfidf_authors = weightTfIdf(DTM_authors)
X = as.matrix(tfidf_authors)
summary(colSums(X))
scrub_cols = which(colSums(X) == 0)
X = X[,-scrub_cols]
pca_authors = prcomp(X, scale=TRUE)
pve = summary(pca_authors)$importance[3,]
plot(pve)  # not much of an elbow
## 75% variance is explained at PC 331
X_pca = pca_authors$x[,1:331]
y = rep(author_names,each=50)
train = as.data.frame(cbind(X_pca,y))
train$y = as.factor(train$y)
## Random Forest Classifier
library(randomForest)
rf_classifier <- randomForest(y ~ .,data=train,ntree=100,mtry=2,importance=TRUE)
rf_classifier <- randomForest(y~.,data=train,ntree=100,mtry=2)
predict(rf_classifier)
rf_classifier$importance
varImpPlot(rf_classifier)
## Test Dataset
setwd("C:/Users/prakh/Desktop/GitHub/STA380/data/ReutersC50/C50test")
file_list = Sys.glob('*/*.txt')
authors = lapply(file_list, readerPlain)
author_names = Sys.glob('*')
mynames = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
# Rename the articles
names(authors) = mynames
## once you have documents in a vector, you
## create a text mining 'corpus' with:
documents_raw = Corpus(VectorSource(authors))
## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus
my_documents = documents_raw %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
## Remove stopwords.  Always be careful with this: one person's trash is another one's treasure.
# 2 example built-in sets of stop words
stopwords("en")
stopwords("SMART")
?stopwords
# let's just use the "basic English" stop words
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
## create a doc-term-matrix from the corpus
DTM_authors = DocumentTermMatrix(my_documents)
DTM_authors # some basic summary statistics
## You can inspect its entries...
inspect(DTM_authors[1:10,1:20])
## ...find words with greater than a min count...
findFreqTerms(DTM_authors, 50)
## Finally, let's drop those terms that only occur in one or two documents
## This is a common step: the noise of the "long tail" (rare terms)
## can be huge, and there is nothing to learn if a term occured once.
## Below removes those terms that have count 0 in >95% of docs.
## Probably a bit stringent here... but only 50 docs!
DTM_authors = removeSparseTerms(DTM_authors, 0.95)
DTM_authors # now ~ 1000 terms (versus ~3000 before)
# construct TF IDF weights -- might be useful if we wanted to use these
# as features in a predictive model
tfidf_authors = weightTfIdf(DTM_authors)
X = as.matrix(tfidf_authors)
summary(colSums(X))
scrub_cols = which(colSums(X) == 0)
X = X[,-scrub_cols]
pca_authors = prcomp(X, scale=TRUE)
pve = summary(pca_authors)$importance[3,]
plot(pve)  # not much of an elbow
## 75% variance is explained at PC 331
X_pca_test = pca_authors$x[,1:331]
y_test = rep(author_names,each=50)
test = as.data.frame(cbind(X_pca_test,y_test))
# Predictions
predictions = predict(rf_classifier, X_pca_test)
rmse = sqrt(sum((y_test-predictions)^2)/nrow(test))
cat('rmse: ',finrfrmse,'\n')
cat('rmse: ',rmse,'\n')
predictions
## 75% variance is explained at PC 331
X_pca = pca_authors$x[,1:331]
y = rep(author_names,each=50)
train = as.data.frame(cbind(X_pca,y))
rf_classifier = randomForest(y~.,data=train,ntree=100,mtry=2)
train$y = as.factor(train$y)
rf_classifier = randomForest(y~.,data=train,ntree=100,mtry=2)
# Predictions
predictions = predict(rf_classifier, X_pca_test)
predictions
y_test
test$y_test = as.factor(test$y_test)
rmse = sqrt(sum((test$y_test-predictions)^2)/nrow(test))
cat('rmse: ',rmse,'\n')
test$y_test = as.factor(test$y_test)
rmse = sqrt(sum((test$y_test-predictions)^2)/nrow(test))
y_test-predictions
y_test==predictions
sum(y_test==predictions)
sum(y_test==predictions)/length(y_test)
train_pred = predict(rf_classifier,X_pca)
cat('Train Accuracy: ',sum(train$y==train_pred)/length(train))
rf_classifier = randomForest(y~.,data=train,ntree=500,mtry=2)
train_pred = predict(rf_classifier,X_pca)
cat('Train Accuracy: ',sum(train$y==train_pred)/length(train))
## 75% variance is explained at PC 331
X_pca = pca_authors$x[,1:501]
y = rep(author_names,each=50)
train = as.data.frame(cbind(X_pca,y))
train$y = as.factor(train$y)
rf_classifier = randomForest(y~.,data=train,ntree=500,mtry=2)
train_pred = predict(rf_classifier,X_pca)
cat('Train Accuracy: ',sum(train$y==train_pred)/length(train))
## 75% variance is explained at PC 331
X_pca = pca_authors$x[,1:331]
y = rep(author_names,each=50)
train = as.data.frame(cbind(X_pca,y))
train$y = as.factor(train$y)
rf_classifier = randomForest(y~.,data=train,ntree=1000)
train_pred = predict(rf_classifier,X_pca)
cat('Train Accuracy: ',sum(train$y==train_pred)/length(train))
# Predictions
predictions = predict(rf_classifier, X_pca_test)
cat('Accuracy: ',sum(y_test==predictions)/length(y_test))
setwd("C:/Users/prakh/Desktop/Intro to ML/Assignment 2")
knitr::opts_chunk$set(echo = TRUE)
groceries = read.csv('groceries.txt)
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
setwd("C:/Users/prakh/Desktop/Intro to ML/Assignment 2")
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
playlists_raw = read.csv("groceries.txt")
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
playlists_raw = read.table("groceries.txt",sep=',')
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
playlists_raw = read.table("groceries.txt")
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
playlists_raw = read.table("groceries.txt",header = F)
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
playlists_raw = read.table("groceries.txt",header = F,fill=TRUE)
playlists_raw
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
playlists_raw = read.table("groceries.txt",header = F,fill=TRUE,sep=',')
playlists_raw
View(playlists_raw)
str(playlists_raw)
summary(playlists_raw)
str(groceries_raw)
summary(groceries_raw)
musicrules = apriori(groceries_raw,
parameter=list(support=.005, confidence=.1, maxlen=5))
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
musicrules = apriori(groceries_raw,
parameter=list(support=.005, confidence=.1, maxlen=5))
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
groceries_raw = read.table("groceries.txt",header = F,fill=TRUE,sep=',')
str(groceries_raw)
summary(groceries_raw)
musicrules = apriori(groceries_raw,
parameter=list(support=.005, confidence=.1, maxlen=5))
View(musicrules)
# Look at the output... so many rules!
inspect(musicrules)
## Choose a subset
inspect(subset(musicrules, subset=lift > 5))
inspect(subset(musicrules, subset=confidence > 0.6))
inspect(subset(musicrules, subset=lift > 10 & confidence > 0.5))
# plot all the rules in (support, confidence) space
# notice that high lift rules tend to have low support
plot(musicrules)
# can swap the axes and color scales
plot(musicrules, measure = c("support", "lift"), shading = "confidence")
# can now look at subsets driven by the plot
inspect(subset(musicrules, support > 0.035))
inspect(subset(musicrules, confidence > 0.7))
# graph-based visualization
sub1 = subset(musicrules, subset=confidence > 0.01 & support > 0.005)
summary(sub1)
plot(sub1, method='graph')
?plot.rules
plot(head(sub1, 100, by='lift'), method='graph')
# export
saveAsGraph(head(musicrules, n = 1000, by = "lift"), file = "musicrules.graphml")
setwd("C:/Users/prakh/Desktop/GitHub/data_viz_unsupervised_algo")
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
# Association rule mining
# Adapted from code by Matt Taddy
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
groceries_raw = read.table("groceries.txt",header = F,fill=TRUE,sep=',')
str(groceries_raw)
summary(groceries_raw)
# First create a list of baskets: vectors of items by consumer
# Analagous to bags of words
# apriori algorithm expects a list of baskets in a special format
# In this case, one "basket" of songs per user
# First split data into a list of artists for each user
playlists = split(x=groceries_raw$artist, f=playlists_raw$user)
library(tidyverse)
library(tidyverse)
library(arulesViz)
groceries_raw = read.table("groceries.txt",header = F,fill=TRUE,sep=',')
library(knitr)
purl("Prakhar_STA380.Rmd")
