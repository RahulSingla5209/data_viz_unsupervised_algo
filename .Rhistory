all_simulation = rbind(all_simulation, wealth_tracker)
}
wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, 25)
hist(wealth_gain, breaks = 30)
n_simulations = 1000
wealth_tracker = rep(0, n_days)
all_simulation = NULL
for (simulation in 1:n_simulations)
{
current_holding = holding
wealth_tracker = rep(0, n_days)
for (i in 1: n_days)
{
holdings = current_holding * protfolio1
r = resample(all_returns, size = 1)
total_holding = sum((1 + r) * holdings)
wealth_tracker[i] = total_holding
current_holding = total_holding
}
all_simulation = rbind(all_simulation, wealth_tracker)
}
wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 30)
hist(wealth_gain, breaks = 30) + xlim(-5000, 5000)
hist(wealth_gain, breaks = 30, xlim = (-5000, 5000))
hist(wealth_gain, breaks = 30, xlim = c(-5000, 5000))
hist(wealth_gain, breaks = 30, xlim = c(-50000, 50000))
hist(wealth_gain, breaks = 60, xlim = c(-50000, 50000))
holding = 100000 #100 Grand
n_days = 20
protfolio1 = c(0.25, 0.25, 0.25, 0.25)
n_simulations = 10000
wealth_tracker = rep(0, n_days)
all_simulation = NULL
for (simulation in 1:n_simulations)
{
current_holding = holding
wealth_tracker = rep(0, n_days)
for (i in 1: n_days)
{
holdings = current_holding * protfolio1
r = resample(all_returns, size = 1)
total_holding = sum((1 + r) * holdings)
wealth_tracker[i] = total_holding
current_holding = total_holding
}
all_simulation = rbind(all_simulation, wealth_tracker)
}
wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 60, xlim = c(-50000, 50000))
holding = 100000 #100 Grand
n_days = 20
protfolio1 = c(0.25, 0.25, 0.25, 0.25)
n_simulations = 10000
wealth_tracker = rep(0, n_days)
all_simulation = NULL
for (simulation in 1:n_simulations)
{
current_holding = holding
wealth_tracker = rep(0, n_days)
for (i in 1: n_days)
{
holdings = current_holding * protfolio1
r = resample(all_returns, size = 1)
total_holding = sum((1 + r) * holdings)
wealth_tracker[i] = total_holding
current_holding = total_holding
}
all_simulation = rbind(all_simulation, wealth_tracker)
}
wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 100, xlim = c(-50000, 50000))
holding = 100000 #100 Grand
n_days = 20
protfolio1 = c(0.25, 0.25, 0.25, 0.25)
n_simulations = 10000
wealth_tracker = rep(0, n_days)
all_simulation = NULL
for (simulation in 1:n_simulations)
{
current_holding = holding
wealth_tracker = rep(0, n_days)
for (i in 1: n_days)
{
holdings = current_holding * protfolio1
r = resample(all_returns, size = 1)
total_holding = sum((1 + r) * holdings)
wealth_tracker[i] = total_holding
current_holding = total_holding
}
all_simulation = rbind(all_simulation, wealth_tracker)
}
wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 20, xlim = c(-100000, 100000))
holding = 100000 #100 Grand
n_days = 20
protfolio1 = c(0.25, 0.25, 0.25, 0.25)
n_simulations = 100000
wealth_tracker = rep(0, n_days)
all_simulation = NULL
for (simulation in 1:n_simulations)
{
current_holding = holding
wealth_tracker = rep(0, n_days)
for (i in 1: n_days)
{
holdings = current_holding * protfolio1
r = resample(all_returns, size = 1)
total_holding = sum((1 + r) * holdings)
wealth_tracker[i] = total_holding
current_holding = total_holding
}
all_simulation = rbind(all_simulation, wealth_tracker)
}
holding = 100000 #100 Grand
n_days = 20
protfolio1 = c(0.25, 0.25, 0.25, 0.25)
n_simulations = 30000
wealth_tracker = rep(0, n_days)
all_simulation = NULL
for (simulation in 1:n_simulations)
{
current_holding = holding
wealth_tracker = rep(0, n_days)
for (i in 1: n_days)
{
holdings = current_holding * protfolio1
r = resample(all_returns, size = 1)
total_holding = sum((1 + r) * holdings)
wealth_tracker[i] = total_holding
current_holding = total_holding
}
all_simulation = rbind(all_simulation, wealth_tracker)
}
wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 1000, xlim = c(-100000, 100000))
hist(wealth_gain, breaks = 100, xlim = c(-100000, 100000))
hist(wealth_gain, breaks = 100, xlim = c(-50000, 50000))
hist(wealth_gain, breaks = 1000, xlim = c(-50000, 50000))
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000))
quantile(wealth_gain, prob=0.05)
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000),
main = "Gain for Portofolio 1")
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000),
main = "Gain for Portofolio 1")
abline(v = quantile(wealth_gain, prob=0.05))
abline(v = quantile(wealth_gain, prob=0.05), c = 'red)
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000),
main = "Gain for Portofolio 1")
abline(v = quantile(wealth_gain, prob=0.05), c = 'red)
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000),
main = "Gain for Portofolio 1")
abline(v = quantile(wealth_gain, prob=0.05), color = 'red)
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000),
main = "Gain for Portofolio 1")
abline(v = quantile(wealth_gain, prob=0.05), col = 'red)
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000),
main = "Gain for Portofolio 1")
abline(v = quantile(wealth_gain, prob=0.05), col = 'red')
holding = 100000 #100 Grand
n_days = 20
protfolio1 = c(0.2, 0.15, 0.15, 0.5)
n_simulations = 30000
wealth_tracker = rep(0, n_days)
all_simulation = NULL
for (simulation in 1:n_simulations)
{
current_holding = holding
wealth_tracker = rep(0, n_days)
for (i in 1: n_days)
{
holdings = current_holding * protfolio1
r = resample(all_returns, size = 1)
total_holding = sum((1 + r) * holdings)
wealth_tracker[i] = total_holding
current_holding = total_holding
}
all_simulation = rbind(all_simulation, wealth_tracker)
}
wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000),
main = "Gain for Portofolio 2")
abline(v = quantile(wealth_gain, prob=0.05), col = 'red')
quantile(wealth_gain, prob=0.05)
holding = 100000 #100 Grand
n_days = 20
protfolio1 = c(0.2, 0.2, 0.5, 0.1)
n_simulations = 30000
wealth_tracker = rep(0, n_days)
all_simulation = NULL
for (simulation in 1:n_simulations)
{
current_holding = holding
wealth_tracker = rep(0, n_days)
for (i in 1: n_days)
{
holdings = current_holding * protfolio1
r = resample(all_returns, size = 1)
total_holding = sum((1 + r) * holdings)
wealth_tracker[i] = total_holding
current_holding = total_holding
}
all_simulation = rbind(all_simulation, wealth_tracker)
}
wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000),
main = "Gain for Portofolio 3")
abline(v = quantile(wealth_gain, prob=0.05), col = 'red')
quantile(wealth_gain, prob=0.05)
rm(list = ls())
df = read.csv('social_marketing.csv', header = T)
hist(c(1, 4, 5, 6, 15, 16, 17, 20, 25, 30))
x = c(1, 4, 5, 6, 15, 16, 17, 20, 25, 30)
plot(x, c(1:10))
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(tm)
library(tidyverse)
library(slam)
library(proxy)
library(plyr)
library(caret)
library(randomForest)
# function for reading text as plain data
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
# fetching all directory
file_list = Sys.glob('./ReutersC50/C50train/*/*.txt')
# read text
text = lapply(file_list, readerPlain)
# Create a corpus from the text above
documents_raw = Corpus(VectorSource(text))
# modifying corpus
my_documents = documents_raw %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
# removing stop words
my_documents = tm_map(my_documents,
content_transformer(removeWords),
stopwords("en"))
# Create Term frequency matrix. Each row is term frequncy vector of single  document
DTM = DocumentTermMatrix(my_documents)
# Current Sparisty is 99%. Let's remove long tail of infrequently used words
DTM = removeSparseTerms(DTM, 0.95)
train_X = as.matrix(DTM)
# Let's create target variable
train_Y = rep(1, 50)
for (i in 2:50)
{
train_Y = c(train_Y, rep(i, 50))
}
# fetching all directory
file_list = Sys.glob('./ReutersC50/C50test/*/*.txt')
# read text
text = lapply(file_list, readerPlain)
# Create a corpus from the text above
documents_raw = Corpus(VectorSource(text))
# modifying corpus
my_documents = documents_raw %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
# removing stop words
my_documents = tm_map(my_documents,
content_transformer(removeWords),
stopwords("en"))
# Create Term frequency matrix. Each row is term frequncy vector of single  document
DTM = DocumentTermMatrix(my_documents)
# Current Sparisty is 99%. Let's remove long tail of infrequently used words
DTM = removeSparseTerms(DTM, 0.95)
test_X = as.matrix(DTM)
# Let's create target variable
test_Y = rep(1, 50)
for (i in 2:50)
{
test_Y = c(test_Y, rep(i, 50))
}
scrub_cols = which(colSums(train_X) == 0)
scrub_cols = which(sapply(as.data.frame(train_X), function(x) var(x, na.rm = T)) == 0)
train_X = train_X[,-scrub_cols]
pr.out=prcomp(train_X , scale=TRUE, rank = 20)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(tm)
library(tidyverse)
library(slam)
library(proxy)
library(plyr)
library(caret)
library(randomForest)
# function for reading text as plain data
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
# fetching all directory
file_list = Sys.glob('./ReutersC50/C50train/*/*.txt')
# read text
text = lapply(file_list, readerPlain)
# Create a corpus from the text above
documents_raw = Corpus(VectorSource(text))
# modifying corpus
my_documents = documents_raw %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
# removing stop words
my_documents = tm_map(my_documents,
content_transformer(removeWords),
stopwords("en"))
# Create Term frequency matrix. Each row is term frequncy vector of single  document
DTM = DocumentTermMatrix(my_documents)
# Current Sparisty is 99%. Let's remove long tail of infrequently used words
DTM = removeSparseTerms(DTM, 0.95)
train_X = as.matrix(DTM)
# Let's create target variable
train_Y = rep(1, 50)
for (i in 2:50)
{
train_Y = c(train_Y, rep(i, 50))
}
# fetching all directory
file_list = Sys.glob('./ReutersC50/C50test/*/*.txt')
# read text
text = lapply(file_list, readerPlain)
# Create a corpus from the text above
documents_raw = Corpus(VectorSource(text))
# modifying corpus
my_documents = documents_raw %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
# removing stop words
my_documents = tm_map(my_documents,
content_transformer(removeWords),
stopwords("en"))
# Create Term frequency matrix. Each row is term frequncy vector of single  document
DTM = DocumentTermMatrix(my_documents)
# Current Sparisty is 99%. Let's remove long tail of infrequently used words
DTM = removeSparseTerms(DTM, 0.95)
test_X = as.matrix(DTM)
# Let's create target variable
test_Y = rep(1, 50)
for (i in 2:50)
{
test_Y = c(test_Y, rep(i, 50))
}
scrub_cols = which(colSums(train_X) == 0)
scrub_cols = which(sapply(as.data.frame(train_X), function(x) var(x, na.rm = T)) == 0)
train_X = train_X[,-scrub_cols]
pr.out=prcomp(train_X , scale=TRUE, rank = 20)
rf = randomForest(x = pr.out$x, y = as.factor(train_Y),
type = 'classification', maxnodes = 300)
confusionMatrix(rf$predicted, as.factor(train_Y),
positive = NULL,
dnn = c("Prediction", "Reference"))
cols = c()
for (i in c(1: length(test_Y)))
{
if(colnames(test_X)[i] %in% colnames(train_X))
cols = c(cols, i)
}
test_scrubbed = scale(test_X)[,cols]
yy <- read.table(textConnection(""), col.names = colnames(train_X),
colClasses = "integer")
test_data = rbind.fill(yy, as.data.frame(test_scrubbed))
test_data[is.na(test_data)] = 0
test_pca = predict(pr.out, newdata = test_data)
test_result = predict(rf, newdata = test_pca, type = 'class')
confusionMatrix(test_result, as.factor(train_Y),
positive = NULL,
dnn = c("Prediction", "Reference"))
summary(pr.out)
pr.out=prcomp(train_X , scale=TRUE, rank = 60)
pr.out=prcomp(train_X , scale=TRUE, rank = 60)
summary(pr.out)
pr.out=prcomp(train_X , scale=TRUE, rank = 100)
summary(pr.out)
pr.out=prcomp(train_X , scale=TRUE, rank = 200)
summary(pr.out)
rf = randomForest(x = pr.out$x, y = as.factor(train_Y),
type = 'classification', maxnodes = 300)
confusionMatrix(rf$predicted, as.factor(train_Y),
positive = NULL,
dnn = c("Prediction", "Reference"))
cols = c()
for (i in c(1: length(test_Y)))
{
if(colnames(test_X)[i] %in% colnames(train_X))
cols = c(cols, i)
}
test_scrubbed = scale(test_X)[,cols]
yy <- read.table(textConnection(""), col.names = colnames(train_X),
colClasses = "integer")
test_data = rbind.fill(yy, as.data.frame(test_scrubbed))
test_data[is.na(test_data)] = 0
test_pca = predict(pr.out, newdata = test_data)
test_result = predict(rf, newdata = test_pca, type = 'class')
confusionMatrix(test_result, as.factor(train_Y),
positive = NULL,
dnn = c("Prediction", "Reference"))
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
playlists_raw = read.csv("../data/playlists.csv")
setwd("C:/Studies/Intro to ML/second half/R")
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
playlists_raw = read.csv("../data/playlists.csv")
str(playlists_raw)
summary(playlists_raw)
rm(list = ls())
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
playlists_raw = read.csv("../data/playlists.csv")
str(playlists_raw)
summary(playlists_raw)
View(playlists_raw)
# Barplot of top 20 artists
# Cool use of magrittr pipes in plotting/summary workflow
# the dot (.) means "plug in the argument coming from the left"
playlists_raw$artist %>%
summary(., maxsum=Inf) %>%
sort(., decreasing=TRUE) %>%
head(., 20) %>%
barplot(., las=2, cex.names=0.6)
# Turn user into a factor
playlists_raw$user = factor(playlists_raw$user)
# apriori algorithm expects a list of baskets in a special format
# In this case, one "basket" of songs per user
# First split data into a list of artists for each user
playlists = split(x=playlists_raw$artist, f=playlists_raw$user)
View(playlists)
## Remove duplicates ("de-dupe")
playlists = lapply(playlists, unique)
## Cast this variable as a special arules "transactions" class.
playtrans = as(playlists, "transactions")
View(playtrans)
summary(playtrans)
# Now run the 'apriori' algorithm
# Look at rules with support > .005 & confidence >.1 & length (# artists) <= 5
musicrules = apriori(playtrans,
parameter=list(support=.005, confidence=.1, maxlen=5))
# Look at the output... so many rules!
inspect(musicrules)
## Choose a subset
inspect(subset(musicrules, subset=lift > 5))
inspect(subset(musicrules, subset=confidence > 0.6))
inspect(subset(musicrules, subset=lift > 10 & confidence > 0.5))
# plot all the rules in (support, confidence) space
# notice that high lift rules tend to have low support
plot(musicrules)
# can swap the axes and color scales
plot(musicrules, measure = c("support", "lift"), shading = "confidence")
# "two key" plot: coloring is by size (order) of item set
plot(musicrules, method='two-key plot')
# can now look at subsets driven by the plot
inspect(subset(musicrules, support > 0.035))
inspect(subset(musicrules, confidence > 0.7))
# graph-based visualization
sub1 = subset(musicrules, subset=confidence > 0.01 & support > 0.005)
summary(sub1)
plot(sub1, method='graph')
?plot.rules
plot(head(sub1, 100, by='lift'), method='graph')
# export
saveAsGraph(head(musicrules, n = 1000, by = "lift"), file = "musicrules.graphml")
setwd("C:/Studies/Intro to ML/group assignment 2")
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
groceries = read.csv('groceries.txt', sep = ' ')
groceries = read.csv('groceries.txt', sep = ',', header = F)
View(groceries)
View(groceries)
groceries = read.transactions('groceries.txt', sep = ',')
View(groceries)
summary(groceries)
groceries = read.transactions('groceries.txt', sep = ',',
rm.duplicates = T, header = F,
format = 'basket')
groceries = read.transactions('groceries.txt', sep = ',',
rm.duplicates = T, header = F,
format = 'basket')
summary(groceries)
grocery_rules = apriori(groceries,
parameter=list(support=.005, confidence=.1, maxlen=5))
inspect(grocery_rules)
grocery_rules = apriori(groceries,
parameter=list(support=.005, confidence=.1, lift=2))
inspect(subset(grocery_rules, subset=lift > 5))
grocery_rules = apriori(groceries,
parameter=list(support=.005, confidence=.1, maxlen=5))
inspect(subset(grocery_rules, subset=lift > 5))
inspect(subset(grocery_rules, subset=lift > 2))
inspect(subset(grocery_rules, subset=lift > 3))
inspect(subset(grocery_rules, subset=lift > 4))
plot(grocery_rules)
plot(musicrules, measure = c("support", "lift"), shading = "confidence")
plot(grocery_rules, measure = c("support", "lift"), shading = "confidence")
plot(grocery_rules, measure = c("support", "confidence"), shading = 'lift')
saveAsGraph(head(grocery_rules, n = 1000, by = "lift"), file = "musicrules.graphml")
