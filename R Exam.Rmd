---
title: "R Exam"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1
```{r, echo=FALSE}
rm(list = ls())
library(DescTools)
library(tidyverse)
df = read.csv('greenbuildings.csv', header = T)

df$green_rating = as.factor(df$green_rating)
df$scaled_rent = df$Rent / df$cluster_rent

#scaled
difference_in_rents =
    median(df[df$green_rating  == 1,]$scaled_rent) - 
            median(df[df$green_rating  == 0,]$scaled_rent)
```
Rents of Green housing is, on average, `r difference_in_rents`% greater than non-green

```{r, echo=FALSE}
additional_rent_pre_sqft = 25 * (difference_in_rents + 1) #assuming $25 to be base

area = 250000 #sqft
additional_pay_from_green = additional_rent_pre_sqft * area
```
Additional Pay from Green - `r additional_pay_from_green`



```{r, echo=FALSE}
#df %>% ggplot(aes(x = stories, y = Rent)) + 
#  geom_point() +
#  geom_vline(xintercept = 15) +
#  stat_smooth(method = "lm", col = "red", formula = 'y ~ x') +
#  facet_wrap(vars(green_rating)) + xlim(0, 60)
#
#df %>% ggplot(aes(x = size, y = Rent, color = class_a)) + 
#  geom_point() +
#  geom_vline(xintercept = 250000) +
#  stat_smooth(method = "lm", col = "red", formula = 'y ~ x') +
#  facet_wrap(vars(green_rating)) + scale_x_log10() + ylim(0, 100)

df$class_a = as.factor(df$class_a)
df %>% ggplot(aes(x = Rent, color = green_rating)) + 
  geom_histogram() +  facet_wrap(vars(class_a), nrow = 2) + 
  xlim(0, 100) +
  labs(title = "Facet wrap - Class A")

df %>% ggplot(aes(x = Rent, color = green_rating)) + 
  geom_histogram() +  
  facet_wrap(vars(amenities), nrow = 2) + 
  xlim(0, 100) +
  labs(title = "Facet wrap - Amenties")
```


**The East Cesar Chavez area seems to be high profile and seems like that it has all the amenities. Let's rerun the analysis for class a and amenties**

```{r}
df1 = df[df$class_a == 1,]
additional_rent_pre_sqft = median(df1[df1$green_rating  == 1,]$Rent) - median(df1[df1$green_rating  == 0,]$Rent)
area = 250000 #sqft
additional_pay_from_green = additional_rent_pre_sqft * area
```
Additional Pay from Green - `r additional_pay_from_green`

```{r}
df2 = df[df$amenities == 1,]
additional_rent_pre_sqft = median(df2[df2$green_rating  == 1,]$Rent) - median(df2[df2$green_rating  == 0,]$Rent)

area = 250000 #sqft
additional_pay_from_green = additional_rent_pre_sqft * area
```
Additional Pay from Green - `r additional_pay_from_green`

```{r}
df3 = df[df$amenities == 1 & df$class_a == 1,]
additional_rent_pre_sqft = median(df3[df3$green_rating  == 1,]$Rent) - median(df3[df3$green_rating  == 0,]$Rent)

area = 250000 #sqft
additional_pay_from_green = additional_rent_pre_sqft * area
```
Additional Pay from Green - `r additional_pay_from_green`




# Question 2

```{r, echo=FALSE}
rm(list = ls())
df = data.table::fread('ABIA.csv')
df = df[df$Cancelled == 0,]
df[is.na(df)] <- 0
df$unavoidable_delays = df$WeatherDelay + df$NASDelay + df$SecurityDelay
df$unavoidable_delays_perc = abs(df$unavoidable_delays / df$DepDelay) > 0
df[is.na(df)] <- 0
df%>%ggplot() + 
  geom_point(aes(x=ArrDelay, y = DepDelay, 
                  shape = as.factor(unavoidable_delays_perc),
                  color = as.factor(unavoidable_delays_perc)),
             size=1.5,
             alpha = 0.3) + 
  facet_wrap(vars(Month)) +
  labs(title = "Arrival Delay vs Dept Delay by months",
       x = "Arrival Delay",
       y = "Departure Delay",
       color = "Delay caused due to \n Airport Authorities",
       shape = "Delay caused due to \n Airport Authorities"
       )
```



```{r, echo=FALSE}
df1 <-  df %>% 
  group_by(Month) %>%
  summarise("average_dep_delay" = mean(DepDelay),
            "average_unavoidable_delays" = mean(unavoidable_delays, na.rm = T),
            "percentage" = average_unavoidable_delays / average_dep_delay) %>% ungroup()
  
df1 %>% 
  ggplot(aes(y=average_dep_delay, 
             x = Month, 
             fill = average_unavoidable_delays)) + 
  geom_bar(position="stack", stat="identity")
```


# Question 3

```{r, echo=FALSE}
rm(list = ls())


library(quantmod)
library(mosaic)

#stocks chosen - 
# Low Risk - Apple, Google, Texas Instruments 
# Medium Risk - Tesla

mystocks = c("AAPL",  "GOOGL", "TXN", "TSLA")
getSymbols(mystocks, from = "2016-01-01")

#adjust for stock splits and dividends 
apple = adjustOHLC(AAPL)
google = adjustOHLC(GOOGL)
ti = adjustOHLC(TXN)
tesla = adjustOHLC(TSLA)

df = cbind(ClCl(apple), ClCl(google), ClCl(ti), 
           ClCl(tesla))
all_returns = as.matrix(na.omit(df))
```


## Portofolio 1
```{r, echo=FALSE}
holding = 100000 #100 Grand
n_days = 20

protfolio1 = c(0.25, 0.25, 0.25, 0.25)

n_simulations = 30000

wealth_tracker = rep(0, n_days)
all_simulation = NULL

for (simulation in 1:n_simulations)
{
  current_holding = holding
  wealth_tracker = rep(0, n_days)
  for (i in 1: n_days)
  {
    holdings = current_holding * protfolio1
    r = resample(all_returns, size = 1)
    total_holding = sum((1 + r) * holdings)
    wealth_tracker[i] = total_holding
    current_holding = total_holding
  }
  all_simulation = rbind(all_simulation, wealth_tracker)
}

wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000), 
     main = "Gain for Portofolio 1")
abline(v = quantile(wealth_gain, prob=0.05), col = 'red')
```


`r quantile(wealth_gain, prob=0.05)`

## Portofolio 2
```{r, echo=FALSE}
holding = 100000 #100 Grand
n_days = 20

protfolio1 = c(0.2, 0.15, 0.15, 0.5)

n_simulations = 30000

wealth_tracker = rep(0, n_days)
all_simulation = NULL

for (simulation in 1:n_simulations)
{
  current_holding = holding
  wealth_tracker = rep(0, n_days)
  for (i in 1: n_days)
  {
    holdings = current_holding * protfolio1
    r = resample(all_returns, size = 1)
    total_holding = sum((1 + r) * holdings)
    wealth_tracker[i] = total_holding
    current_holding = total_holding
  }
  all_simulation = rbind(all_simulation, wealth_tracker)
}

wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000), 
     main = "Gain for Portofolio 2")
abline(v = quantile(wealth_gain, prob=0.05), col = 'red')
```


`r quantile(wealth_gain, prob=0.05)`

## Portofolio 3
```{r, echo=FALSE}
holding = 100000 #100 Grand
n_days = 20

protfolio1 = c(0.2, 0.2, 0.5, 0.1)

n_simulations = 30000

wealth_tracker = rep(0, n_days)
all_simulation = NULL

for (simulation in 1:n_simulations)
{
  current_holding = holding
  wealth_tracker = rep(0, n_days)
  for (i in 1: n_days)
  {
    holdings = current_holding * protfolio1
    r = resample(all_returns, size = 1)
    total_holding = sum((1 + r) * holdings)
    wealth_tracker[i] = total_holding
    current_holding = total_holding
  }
  all_simulation = rbind(all_simulation, wealth_tracker)
}

wealth_gain = all_simulation[, 20] - holding
hist(wealth_gain, breaks = 1000, xlim = c(-40000, 40000), 
     main = "Gain for Portofolio 3")
abline(v = quantile(wealth_gain, prob=0.05), col = 'red')
```


`r quantile(wealth_gain, prob=0.05)`

# Question 4

```{r, echo=FALSE}
rm(list = ls())
library(LICORS)

df = read.csv('social_marketing.csv', header = T)

df = df[, -1]
```

number of followers - `r nrow(df)`

## K-Mean clustering

K-Means good when the number of samples is huge

No scaling will be done because the unit of all the columns is same.

Different K and different distance metrics will be explored, starting with K = 5 and Euclidean distance metric.

```{r, echo=FALSE}
clusters = c(2: 15)
within_SSE = rep(0, length(clusters))
between_SSE = rep(0, length(clusters))

for (i in c(1: length(clusters)))
{
  clust1 = kmeanspp(df, clusters[i])
  within_SSE[i] = clust1$tot.withinss
  between_SSE[i] = clust1$betweenss
}

plot(clusters, within_SSE)
plot(clusters, between_SSE)

```

```{r, echo=FALSE}
set.seed(123)
clust1 = kmeanspp(df, 4)
(clust1$centers - colMeans(df, na.rm = T))/apply(df, 2, sd)
```

Groups identified -
1. Teen girls? - High shopping, fashion, school, chatter, photosharing
2. Fitness freaks - high - Outdoors, personal_fitness , health_nutirition
3. Old person - religion, adult, food
4. college students - high - college_uni, online_gaming, sport_playing



```{r, echo=FALSE}

length(which(clust1$cluster == 1))
length(which(clust1$cluster == 2))
length(which(clust1$cluster == 3))
length(which(clust1$cluster == 4))

```

## Heirarchial Clustering

```{r, echo=FALSE}
hier_people = hclust(dist(df, method='euclidean'), method='complete')
plot(hier_people, cex=0.8)
```


```{r, echo=FALSE}
cluster2 = cutree(hier_people, k = 4)
summary(factor(cluster2))

```


I think KMeans clustering provides good enough insights.


# Question 5

```{r}
rm(list = ls())
library(tm) 
library(tidyverse)
library(slam)
library(proxy)
library(plyr)
library(caret)
library(randomForest)

# function for reading text as plain data
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }

# fetching all directory
file_list = Sys.glob('./ReutersC50/C50train/*/*.txt')

# read text
text = lapply(file_list, readerPlain) 

# Create a corpus from the text above
documents_raw = Corpus(VectorSource(text))

# modifying corpus
my_documents = documents_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))          # remove excess white-space

# removing stop words
my_documents = tm_map(my_documents, 
                      content_transformer(removeWords), 
                      stopwords("en"))

# Create Term frequency matrix. Each row is term frequncy vector of single  document 
DTM = DocumentTermMatrix(my_documents)
# Current Sparisty is 99%. Let's remove long tail of infrequently used words
DTM = removeSparseTerms(DTM, 0.95)

train_X = as.matrix(DTM)

# Let's create target variable
train_Y = rep(1, 50)
for (i in 2:50)
{
  train_Y = c(train_Y, rep(i, 50))
}
```



```{r}
# fetching all directory
file_list = Sys.glob('./ReutersC50/C50test/*/*.txt')

# read text
text = lapply(file_list, readerPlain) 

# Create a corpus from the text above
documents_raw = Corpus(VectorSource(text))

# modifying corpus
my_documents = documents_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))          # remove excess white-space

# removing stop words
my_documents = tm_map(my_documents, 
                      content_transformer(removeWords), 
                      stopwords("en"))

# Create Term frequency matrix. Each row is term frequncy vector of single  document 
DTM = DocumentTermMatrix(my_documents)
# Current Sparisty is 99%. Let's remove long tail of infrequently used words
DTM = removeSparseTerms(DTM, 0.95)

test_X = as.matrix(DTM)

# Let's create target variable
test_Y = rep(1, 50)
for (i in 2:50)
{
  test_Y = c(test_Y, rep(i, 50))
}
```


```{r}
scrub_cols = which(colSums(train_X) == 0)
scrub_cols = which(sapply(as.data.frame(train_X), function(x) var(x, na.rm = T)) == 0)

train_X = train_X[,-scrub_cols]
```

```{r}
pr.out=prcomp(train_X , scale=TRUE, rank = 200)
```


```{r}
rf = randomForest(x = pr.out$x, y = as.factor(train_Y), 
                  type = 'classification', maxnodes = 300)
confusionMatrix(rf$predicted, as.factor(train_Y), 
                positive = NULL, 
                dnn = c("Prediction", "Reference"))
```


```{r}
cols = c()
for (i in c(1: length(test_Y)))
{
  if(colnames(test_X)[i] %in% colnames(train_X))
    cols = c(cols, i)
}
test_scrubbed = scale(test_X)[,cols]

yy <- read.table(textConnection(""), col.names = colnames(train_X),
                 colClasses = "integer")

test_data = rbind.fill(yy, as.data.frame(test_scrubbed))
test_data[is.na(test_data)] = 0

test_pca = predict(pr.out, newdata = test_data)
test_result = predict(rf, newdata = test_pca, type = 'class')

confusionMatrix(test_result, as.factor(train_Y), 
                positive = NULL, 
                dnn = c("Prediction", "Reference"))
```


# Question 6

```{r}
rm(list = ls())

library(arules)
library(arulesViz)
groceries = read.transactions('groceries.txt', sep = ',', 
                              rm.duplicates = T, header = F,
                              format = 'basket')
summary(groceries)
```

```{r}
grocery_rules = apriori(groceries, 
	parameter=list(support=.005, confidence=.1, maxlen=5))

x = arules::inspect(subset(grocery_rules, subset=lift > 3.2 & support < 0.1))
```

```{r}
plot(grocery_rules, measure = c("support", "confidence"), shading = 'lift')
```

```{r}
saveAsGraph(head(subset(grocery_rules, subset=lift > 2 & support < 0.1), n = 1000, by = "lift"), file = "grocery.graphml")

knitr::include_graphics("gephi.png")

```


